{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Amino Acid Seq Data --> creating input Tensor\n",
    "\n",
    "1. Loading the data \n",
    "2. Create a Tokenizer for amino acids\n",
    "3. Create a Tensor object \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. notebook init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import string\n",
    "from typing import Iterable, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "- The data is in .txt file, somewhat in a format for two columns. the first column is species-code and the next one is the amino-acid-seq  \n",
    "- The simplest way to get the data is create lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'X_set.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold the phylogenetic position strings and amino acid sequences\n",
    "specie_code = []\n",
    "amino_acid_sequences = []\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        specie_code.append(parts[0])\n",
    "        amino_acid_sequences.append(parts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['111133333333333333333333333333',\n",
       " '111211333333333333333333333333',\n",
       " '111212333333333333333333333333']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specie_code[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---LSQF--LLMLWVPGSKGEIVLTQSPASVSVSPGERVTISCQASESVGNTYLNWLQQKSGQSPRWLIYQVSKLESGIPARFRGSGSGTDFTFTISRVEAEDVAHYYSQQ-----',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPALVSVSPGERVTISCKASQSVGNTYLSWFRQKPGQSPRGLIYKVSNLPSGVPSRFRGSGAEKDFTLTISRVEAVDGAVYYCAQASYSP',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPASVSVSPGERVTISCKASQSLGNTYLHWFQQKPGQSPRRLIYQVSNLLSGVPSRFSGSGAGKDFSLTISSVEAGDGAVYYCFQGSYDP']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_acid_sequences[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Tokenizer for amino acids\n",
    "\n",
    "- There are 20 amino acids, each letter in the chain represents one of them. \n",
    "- Converting them into 20 tokens, meaning each amino acid would get a number associated with it. \n",
    "- Would also need a special character token, which is \"-\", something related to multiple-sequence-alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Amino Acids: 20\n"
     ]
    }
   ],
   "source": [
    "# Creating a set of all amino-acids\n",
    "\n",
    "amino_acid_set = set()\n",
    "\n",
    "for seq in amino_acid_sequences:\n",
    "    for acid in seq:\n",
    "        if acid != \"-\":\n",
    "            amino_acid_set.add(acid)\n",
    "\n",
    "# 20 amino acids\n",
    "print(f\"Num of Amino Acids: {len(amino_acid_set) }\")\n",
    "amino_acids_list = list(amino_acid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Tokenzer class, which ennodes and decodes an amino acid sequence \n",
    "\n",
    "class Tokenizer:\n",
    "    ''' \n",
    "    To encode and decode any amino acid string\n",
    "    '''\n",
    "    # class attribute \n",
    "    amino_acids = amino_acids_list\n",
    "\n",
    "    def __init__(self, special_tokens = Iterable[str]):\n",
    "        # define a vocab\n",
    "        self.vocab = Tokenizer.amino_acids + list(special_tokens)\n",
    "        # mapping each vocab to a token (a numeric value)\n",
    "        self.token2idx = {token:i for i, token in enumerate(self.vocab)} \n",
    "        # mapping numeric value back to a token\n",
    "        self.idx2token = {i:token for token, i  in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, inputs: Iterable[str]) -> Iterable[int]:\n",
    "        return [self.token2idx[token] for token in inputs]\n",
    "    \n",
    "    def decode(self, inputs: Iterable[int]) -> Iterable[str]:\n",
    "        return [self.idx2token[idx] for idx in inputs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of the Tokenizer. \n",
    "amino_acid_tokenizer = Tokenizer(special_tokens=[\"-\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 amino acids         : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n",
      "First 20 encoded amino acids : [20, 20, 20, 1, 6, 9, 2, 20, 20, 1, 1, 14, 1, 0, 7, 17, 4, 6, 3, 4]\n",
      "First 20 decoded amino acids : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n"
     ]
    }
   ],
   "source": [
    "# let's encode the first amino-acid-sequence and see the first 10 positions\n",
    "print(f\"First 20 amino acids         : {[i for i in amino_acid_sequences[0][0:20]]}\")\n",
    "print(f\"First 20 encoded amino acids : {amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20]}\")\n",
    "print(f\"First 20 decoded amino acids : {amino_acid_tokenizer.decode(amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': 0, 'L': 1, 'F': 2, 'K': 3, 'G': 4, 'H': 5, 'S': 6, 'V': 7, 'N': 8, 'Q': 9, 'R': 10, 'A': 11, 'Y': 12, 'C': 13, 'M': 14, 'D': 15, 'E': 16, 'P': 17, 'I': 18, 'T': 19, '-': 20, '[MASK]': 21}\n"
     ]
    }
   ],
   "source": [
    "print(amino_acid_tokenizer.token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 21]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_acid_tokenizer.encode([\"A\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating a Tensor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{116}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making sure that the size of each amino-acid-seq is same\n",
    "\n",
    "len_amino_acid_seq = set()\n",
    "for seq in amino_acid_sequences:\n",
    "    len_amino_acid_seq.add(len(seq))\n",
    "\n",
    "# this set should have only one value \n",
    "len_amino_acid_seq\n",
    "# perfect! all the seq are 116 character long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_amino_acids_tensor(amino_acid_sequences:list, my_tokenizer:Tokenizer):\n",
    "\n",
    "    amino_acid_tensors = []\n",
    "\n",
    "    for seq in amino_acid_sequences:\n",
    "        amino_acid_tensors.append(torch.Tensor(my_tokenizer.encode(seq)).to(torch.int64))\n",
    "\n",
    "    # stacking them \n",
    "    stacked_tensor =  torch.stack(amino_acid_tensors)\n",
    "\n",
    "    return stacked_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amino_acids_tensor = create_amino_acids_tensor(amino_acid_sequences, amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 20, 20,  ..., 20, 20, 20],\n",
       "        [14, 16,  6,  ..., 12,  6, 17],\n",
       "        [14, 16,  6,  ..., 12, 15, 17],\n",
       "        ...,\n",
       "        [20, 20, 20,  ..., 16, 15, 17],\n",
       "        [20, 20, 20,  ..., 16, 15, 17],\n",
       "        [20, 20, 20,  ..., 16, 15, 17]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amino_acids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1001, 116])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amino_acids_tensor.shape\n",
    "# the shape is 1001 species * 116 amino acids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data_old(input_tensor:torch.Tensor, batch_size:int, mask_token:int):\n",
    "\n",
    "    rows, cols = input_tensor.shape\n",
    "\n",
    "    idx = torch.randint(rows-1, (batch_size,))\n",
    "\n",
    "    input_seqs = []\n",
    "    target_amino_acids = []\n",
    "    mask_positions = []\n",
    "    for i in idx:\n",
    "        # select one amino acid seq\n",
    "        selected_amino_seq = input_tensor[i].clone()\n",
    "        # randomly choose a position to mask\n",
    "        mask_position = torch.randint(cols-1, (1,)) \n",
    "        target_amino_acid = selected_amino_seq[mask_position]\n",
    "        # replace the mask posiiton with mask-token\n",
    "        selected_amino_seq[mask_position] = mask_token\n",
    "        train_input_seq = selected_amino_seq\n",
    "\n",
    "        input_seqs.append(train_input_seq)\n",
    "        target_amino_acids.append(target_amino_acid)\n",
    "        mask_positions.append(mask_position)\n",
    "\n",
    "    return input_seqs, target_amino_acids, mask_positions\n",
    "        \n",
    "\n",
    "# create_training_data(all_amino_acids_tensor, batch_size=64, mask_token=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(input_tensor: torch.Tensor, batch_size: int, mask_token: int):\n",
    "    \"\"\"\n",
    "    Creates masked training data efficiently using vectorized operations.\n",
    "\n",
    "    Args:\n",
    "      input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length)\n",
    "      batch_size (int): The desired batch size.\n",
    "      mask_token (int): The token used for masking.\n",
    "\n",
    "    Returns:\n",
    "      tuple: (input_seqs, target_amino_acids, mask_positions)\n",
    "             - input_seqs: Tensor of shape (batch_size, sequence_length) with masked sequences.\n",
    "             - target_amino_acids: Tensor of shape (batch_size,) containing the masked amino acids.\n",
    "             - mask_positions: Tensor of shape (batch_size,) indicating mask positions.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = input_tensor.shape[0]\n",
    "    seq_len = input_tensor.shape[1]\n",
    "    # Randomly select 'batch_size' rows (amino acid sequences)\n",
    "    idx = torch.randint(rows, size=(batch_size,))\n",
    "    input_seqs = input_tensor[idx].clone()\n",
    "\n",
    "    # Generate random mask positions within each selected sequence\n",
    "    mask_positions = torch.randint(seq_len, size=(batch_size, 1))\n",
    "\n",
    "    # Get the target amino acids at the mask positions\n",
    "    target_amino_acids = input_seqs.gather(1, mask_positions).squeeze()\n",
    "\n",
    "    # Create a mask for the selected positions \n",
    "    mask = torch.zeros(input_seqs.size(), dtype=torch.bool)\n",
    "    mask.scatter_(1, mask_positions, 1)\n",
    "\n",
    "    # Apply the mask to replace the target positions with the mask_token\n",
    "    input_seqs[mask] = mask_token\n",
    "\n",
    "    return input_seqs, target_amino_acids, mask_positions.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs, targets, mask_pos = create_training_data(all_amino_acids_tensor, batch_size=32, mask_token=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 116])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exactly one masked value\n",
    "(input_seqs[0] == 21).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 14,  2, 17, 19,  6,  4, 11,\n",
       "        10, 13, 15, 18,  9, 14, 19,  9,  6, 19,  6,  6,  1,  6, 11,  6,  7,  4,\n",
       "        15, 10,  7, 21, 18, 19, 13, 10, 11,  6,  9,  4, 18, 20,  6,  8,  8,  1,\n",
       "         8,  0, 12,  9,  9,  3, 17,  4,  3, 19, 17,  3,  1,  1, 18, 12, 11, 11,\n",
       "         6,  6,  1,  9,  6,  4, 18,  1,  6, 10,  2,  6, 15,  6,  4,  6,  4, 19,\n",
       "        15, 12, 19,  1, 19, 18,  6,  6,  1,  9, 17, 16, 15,  2, 11, 11, 12, 12,\n",
       "        13,  9,  9,  6, 15,  6, 19, 17])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 39, 114,  22,  26,  32, 103,  45,  58,  74,  12,  43,  17,  85,   5,\n",
       "         24,  37, 100,  47,  86,  72,  60,  87,  83,  33,  10, 109,  97,  95,\n",
       "         81,  96, 107,  72])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19,  8,  9,  6,  7, 11,  6,  9, 10,  1, 10,  6,  6, 20, 19, 10, 11,  6,\n",
       "         4,  6, 17,  6,  6, 19,  1,  9, 10, 18, 10,  6, 12, 19])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MaskedAminoSeqDataset(Dataset):\n",
    "    def __init__(self, input_tensor: torch.Tensor, mask_token: int):\n",
    "            \"\"\"\n",
    "            Dataset for masked amino acid sequence prediction.\n",
    "\n",
    "            Args:\n",
    "            input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length).\n",
    "            mask_token (int): The token used for masking.\n",
    "            \"\"\"\n",
    "            self.input_tensor = input_tensor\n",
    "            self.mask_token = mask_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_tensor.shape[0] \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seqs, target_amino_acids, mask_positions = \\\n",
    "            self._create_training_data(self.input_tensor, batch_size=1, mask_token=self.mask_token)\n",
    "        return input_seqs.squeeze(0), target_amino_acids.squeeze(0), mask_positions.squeeze(0)\n",
    "\n",
    "    def _create_training_data(self, input_tensor: torch.Tensor, batch_size: int, mask_token: int):\n",
    "        \"\"\"\n",
    "        Creates masked training data efficiently using vectorized operations.\n",
    "\n",
    "        Args:\n",
    "        input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length)\n",
    "        batch_size (int): The desired batch size.\n",
    "        mask_token (int): The token used for masking.\n",
    "\n",
    "        Returns:\n",
    "        tuple: (input_seqs, target_amino_acids, mask_positions)\n",
    "            - input_seqs: Tensor of shape (batch_size, sequence_length) with masked sequences.\n",
    "            - target_amino_acids: Tensor of shape (batch_size,) containing the masked amino acids.\n",
    "            - mask_positions: Tensor of shape (batch_size,) indicating mask positions.\n",
    "        \"\"\"\n",
    "        rows = input_tensor.shape[0]\n",
    "        seq_len = input_tensor.shape[1]\n",
    "        # Randomly select 'batch_size' rows (amino acid sequences)\n",
    "        idx = torch.randint(rows, size=(batch_size,))\n",
    "        input_seqs = input_tensor[idx].clone()\n",
    "\n",
    "        # Generate random mask positions within each selected sequence\n",
    "        mask_positions = torch.randint(seq_len, size=(batch_size, 1))\n",
    "\n",
    "        # Get the target amino acids at the mask positions\n",
    "        target_amino_acids = input_seqs.gather(1, mask_positions).squeeze()\n",
    "\n",
    "        # Create a mask for the selected positions \n",
    "        mask = torch.zeros(input_seqs.size(), dtype=torch.bool)\n",
    "        mask.scatter_(1, mask_positions, 1)\n",
    "\n",
    "        # Apply the mask to replace the target positions with the mask_token\n",
    "        input_seqs[mask] = mask_token\n",
    "\n",
    "        return input_seqs, target_amino_acids, mask_positions.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming input_tensor is your tensor of amino acid sequences\n",
    "masked_amino_seq_dataset = MaskedAminoSeqDataset(all_amino_acids_tensor, mask_token=21) # Assuming 0 is your mask token\n",
    "masked_amino_seq_dataloader = DataLoader(masked_amino_seq_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 116])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(i[0].shape)\n",
    "    print(i[1].shape)\n",
    "    print(i[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinPredictor(nn.Module):\n",
    "    def init(self, num_variants, seq_length, num_amino_acids, emd_dim=128, nhead=8, num_layers=3):\n",
    "        super().init()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_amino_acids, emd_dim)\n",
    "        self.pos_encoder = PositionalEncoding(emd_dim, seq_length)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emd_dim, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(emd_dim, num_amino_acids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(amino_acid_tokenizer)\n",
    "emb_dim = 8\n",
    "\n",
    "\n",
    "embed = nn.Embedding(vocab_size, emb_dim)\n",
    "pos_emb = nn.Embedding(vocab_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 116])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 116, 8])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(input_seqs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2) * (-math.log(10000.0) / emb_dim))\n",
    "        pe = torch.zeros(max_len, 1, emb_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, emb_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len, emb_dim) with positional encodings added.\n",
    "        \"\"\"\n",
    "        return x + self.pe[:x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoder = PositionalEncoding(emb_dim, max_len=116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (116) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpos_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seqs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep-learning-genetics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep-learning-genetics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 18\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m        x: Tensor of shape (batch_size, seq_len, emb_dim)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m        Tensor of shape (batch_size, seq_len, emb_dim) with positional encodings added.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (116) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "pos_encoder(embed(input_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
