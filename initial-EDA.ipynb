{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Amino Acid Seq Data --> creating input Tensor\n",
    "\n",
    "1. Loading the data \n",
    "2. Create a Tokenizer for amino acids\n",
    "3. Create a Tensor object \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. notebook init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import string\n",
    "from typing import Iterable, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "- The data is in .txt file, somewhat in a format for two columns. the first column is species-code and the next one is the amino-acid-seq  \n",
    "- The simplest way to get the data is create lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'X_set.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold the phylogenetic position strings and amino acid sequences\n",
    "specie_code = []\n",
    "amino_acid_sequences = []\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        specie_code.append(parts[0])\n",
    "        amino_acid_sequences.append(parts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['111133333333333333333333333333',\n",
       " '111211333333333333333333333333',\n",
       " '111212333333333333333333333333']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specie_code[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---LSQF--LLMLWVPGSKGEIVLTQSPASVSVSPGERVTISCQASESVGNTYLNWLQQKSGQSPRWLIYQVSKLESGIPARFRGSGSGTDFTFTISRVEAEDVAHYYSQQ-----',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPALVSVSPGERVTISCKASQSVGNTYLSWFRQKPGQSPRGLIYKVSNLPSGVPSRFRGSGAEKDFTLTISRVEAVDGAVYYCAQASYSP',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPASVSVSPGERVTISCKASQSLGNTYLHWFQQKPGQSPRRLIYQVSNLLSGVPSRFSGSGAGKDFSLTISSVEAGDGAVYYCFQGSYDP']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_acid_sequences[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Tokenizer for amino acids\n",
    "\n",
    "- There are 20 amino acids, each letter in the chain represents one of them. \n",
    "- Converting them into 20 tokens, meaning each amino acid would get a number associated with it. \n",
    "- Would also need a special character token, which is \"-\", something related to multiple-sequence-alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Amino Acids: 20\n"
     ]
    }
   ],
   "source": [
    "# Creating a set of all amino-acids\n",
    "\n",
    "amino_acid_set = set()\n",
    "\n",
    "for seq in amino_acid_sequences:\n",
    "    for acid in seq:\n",
    "        if acid != \"-\":\n",
    "            amino_acid_set.add(acid)\n",
    "\n",
    "# 20 amino acids\n",
    "print(f\"Num of Amino Acids: {len(amino_acid_set) }\")\n",
    "amino_acids_list = list(amino_acid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Tokenzer class, which ennodes and decodes an amino acid sequence \n",
    "\n",
    "class Tokenizer:\n",
    "    ''' \n",
    "    To encode and decode any amino acid string\n",
    "    '''\n",
    "    # class attribute \n",
    "    amino_acids = amino_acids_list\n",
    "\n",
    "    def __init__(self, special_tokens = Iterable[str]):\n",
    "        # define a vocab\n",
    "        self.vocab = Tokenizer.amino_acids + list(special_tokens)\n",
    "        # mapping each vocab to a token (a numeric value)\n",
    "        self.token2idx = {token:i for i, token in enumerate(self.vocab)} \n",
    "        # mapping numeric value back to a token\n",
    "        self.idx2token = {i:token for token, i  in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, inputs: Iterable[str]) -> Iterable[int]:\n",
    "        return [self.token2idx[token] for token in inputs]\n",
    "    \n",
    "    def decode(self, inputs: Iterable[int]) -> Iterable[str]:\n",
    "        return [self.idx2token[idx] for idx in inputs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of the Tokenizer. \n",
    "amino_acid_tokenizer = Tokenizer(special_tokens=[\"-\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 amino acids         : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n",
      "First 20 encoded amino acids : [20, 20, 20, 1, 11, 5, 13, 20, 20, 1, 1, 7, 1, 4, 6, 10, 0, 11, 15, 0]\n",
      "First 20 decoded amino acids : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n"
     ]
    }
   ],
   "source": [
    "# let's encode the first amino-acid-sequence and see the first 10 positions\n",
    "print(f\"First 20 amino acids         : {[i for i in amino_acid_sequences[0][0:20]]}\")\n",
    "print(f\"First 20 encoded amino acids : {amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20]}\")\n",
    "print(f\"First 20 decoded amino acids : {amino_acid_tokenizer.decode(amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'G': 0, 'L': 1, 'Y': 2, 'T': 3, 'W': 4, 'Q': 5, 'V': 6, 'M': 7, 'N': 8, 'C': 9, 'P': 10, 'S': 11, 'A': 12, 'F': 13, 'D': 14, 'K': 15, 'R': 16, 'I': 17, 'E': 18, 'H': 19, '-': 20, '[MASK]': 21}\n"
     ]
    }
   ],
   "source": [
    "print(amino_acid_tokenizer.token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 21]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_acid_tokenizer.encode([\"A\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating a Tensor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{116}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making sure that the size of each amino-acid-seq is same\n",
    "\n",
    "len_amino_acid_seq = set()\n",
    "for seq in amino_acid_sequences:\n",
    "    len_amino_acid_seq.add(len(seq))\n",
    "\n",
    "# this set should have only one value \n",
    "len_amino_acid_seq\n",
    "# perfect! all the seq are 116 character long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_amino_acids_tensor(amino_acid_sequences:list, my_tokenizer:Tokenizer):\n",
    "\n",
    "    amino_acid_tensors = []\n",
    "\n",
    "    for seq in amino_acid_sequences:\n",
    "        amino_acid_tensors.append(torch.Tensor(my_tokenizer.encode(seq)).to(torch.int64))\n",
    "\n",
    "    # stacking them \n",
    "    stacked_tensor =  torch.stack(amino_acid_tensors)\n",
    "\n",
    "    return stacked_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amino_acids_tensor = create_amino_acids_tensor(amino_acid_sequences, amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 20, 20,  ..., 20, 20, 20],\n",
       "        [ 7, 18, 11,  ...,  2, 11, 10],\n",
       "        [ 7, 18, 11,  ...,  2, 14, 10],\n",
       "        ...,\n",
       "        [20, 20, 20,  ..., 18, 14, 10],\n",
       "        [20, 20, 20,  ..., 18, 14, 10],\n",
       "        [20, 20, 20,  ..., 18, 14, 10]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amino_acids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1001, 116])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amino_acids_tensor.shape\n",
    "# the shape is 1001 species * 116 amino acids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data_old(input_tensor:torch.Tensor, batch_size:int, mask_token:int):\n",
    "\n",
    "    rows, cols = input_tensor.shape\n",
    "\n",
    "    idx = torch.randint(rows-1, (batch_size,))\n",
    "\n",
    "    input_seqs = []\n",
    "    target_amino_acids = []\n",
    "    mask_positions = []\n",
    "    for i in idx:\n",
    "        # select one amino acid seq\n",
    "        selected_amino_seq = input_tensor[i].clone()\n",
    "        # randomly choose a position to mask\n",
    "        mask_position = torch.randint(cols-1, (1,)) \n",
    "        target_amino_acid = selected_amino_seq[mask_position]\n",
    "        # replace the mask posiiton with mask-token\n",
    "        selected_amino_seq[mask_position] = mask_token\n",
    "        train_input_seq = selected_amino_seq\n",
    "\n",
    "        input_seqs.append(train_input_seq)\n",
    "        target_amino_acids.append(target_amino_acid)\n",
    "        mask_positions.append(mask_position)\n",
    "\n",
    "    return input_seqs, target_amino_acids, mask_positions\n",
    "        \n",
    "\n",
    "# create_training_data(all_amino_acids_tensor, batch_size=64, mask_token=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(input_tensor: torch.Tensor, batch_size: int, mask_token: int):\n",
    "    \"\"\"\n",
    "    Creates masked training data efficiently using vectorized operations.\n",
    "\n",
    "    Args:\n",
    "      input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length)\n",
    "      batch_size (int): The desired batch size.\n",
    "      mask_token (int): The token used for masking.\n",
    "\n",
    "    Returns:\n",
    "      tuple: (input_seqs, target_amino_acids, mask_positions)\n",
    "             - input_seqs: Tensor of shape (batch_size, sequence_length) with masked sequences.\n",
    "             - target_amino_acids: Tensor of shape (batch_size,) containing the masked amino acids.\n",
    "             - mask_positions: Tensor of shape (batch_size,) indicating mask positions.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = input_tensor.shape[0]\n",
    "    seq_len = input_tensor.shape[1]\n",
    "    # Randomly select 'batch_size' rows (amino acid sequences)\n",
    "    idx = torch.randint(rows, size=(batch_size,))\n",
    "    input_seqs = input_tensor[idx].clone()\n",
    "\n",
    "    # Generate random mask positions within each selected sequence\n",
    "    mask_positions = torch.randint(seq_len, size=(batch_size, 1))\n",
    "\n",
    "    # Get the target amino acids at the mask positions\n",
    "    target_amino_acids = input_seqs.gather(1, mask_positions).squeeze()\n",
    "\n",
    "    # Create a mask for the selected positions \n",
    "    mask = torch.zeros(input_seqs.size(), dtype=torch.bool)\n",
    "    mask.scatter_(1, mask_positions, 1)\n",
    "\n",
    "    # Apply the mask to replace the target positions with the mask_token\n",
    "    input_seqs[mask] = mask_token\n",
    "\n",
    "    return input_seqs, target_amino_acids, mask_positions.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs, targets, mask_pos = create_training_data(all_amino_acids_tensor, batch_size=32, mask_token=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 116])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exactly one masked value\n",
    "(input_seqs[0] == 21).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  6, 11,  5,  3,  5, 13, 17, 11, 20,  1,  1,  1,  4, 17, 21,  0, 12,\n",
       "         2,  0, 14, 17,  6,  7,  3,  5, 11, 10, 14, 11,  1, 12,  6, 11,  1,  0,\n",
       "        18, 16,  6,  3, 17,  8,  9, 15, 11, 11,  5, 11,  6,  8, 15,  8,  2,  1,\n",
       "         8,  4,  2,  5,  5, 15, 10,  0,  5, 12, 10, 15,  1,  1, 17,  2,  4, 12,\n",
       "        11,  3, 16, 18, 11,  0,  6, 10, 14, 16, 13, 11,  0, 11,  0, 11,  0,  3,\n",
       "        14, 13,  3,  1,  3, 17, 11, 11,  1,  5, 12, 18, 14,  6, 12,  6,  2,  2,\n",
       "         9,  5,  5, 11,  2, 11,  3, 10])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 15,  78,  48, 115,  13,  44,  43,  70,  99, 101,  18,  21,   3,  52,\n",
       "         68, 100,  28, 110,  28,  56,  68,  38,  48,  77,  18,  58,  78,  81,\n",
       "         78,  43, 111, 108])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
