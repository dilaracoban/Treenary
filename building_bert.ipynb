{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model of Amino-Seq Masked-Language-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import string\n",
    "from typing import Iterable, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---LSQF--LLMLWVPGSKGEIVLTQSPASVSVSPGERVTISCQASESVGNTYLNWLQQKSGQSPRWLIYQVSKLESGIPARFRGSGSGTDFTFTISRVEAEDVAHYYSQQ-----',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPALVSVSPGERVTISCKASQSVGNTYLSWFRQKPGQSPRGLIYKVSNLPSGVPSRFRGSGAEKDFTLTISRVEAVDGAVYYCAQASYSP',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPASVSVSPGERVTISCKASQSLGNTYLHWFQQKPGQSPRRLIYQVSNLLSGVPSRFSGSGAGKDFSLTISSVEAGDGAVYYCFQGSYDP']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'X_set.txt'\n",
    "\n",
    "# Initialize lists to hold the phylogenetic position strings and amino acid sequences\n",
    "specie_code = []\n",
    "amino_acid_sequences = []\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        specie_code.append(parts[0])\n",
    "        amino_acid_sequences.append(parts[1])\n",
    "\n",
    "amino_acid_sequences[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tokenizer\n",
    "\n",
    "- There are 20 amino acids, each letter in the chain represents one of them. \n",
    "- Converting them into 20 tokens, meaning each amino acid would get a number associated with it. \n",
    "- Would also need a special character token, which is \"-\", something related to multiple-sequence-alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Amino Acids: 20\n"
     ]
    }
   ],
   "source": [
    "# Creating a set of all amino-acids\n",
    "\n",
    "amino_acid_set = set()\n",
    "\n",
    "for seq in amino_acid_sequences:\n",
    "    for acid in seq:\n",
    "        if acid != \"-\":\n",
    "            amino_acid_set.add(acid)\n",
    "\n",
    "# 20 amino acids\n",
    "print(f\"Num of Amino Acids: {len(amino_acid_set) }\")\n",
    "amino_acids_list = list(amino_acid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Tokenzer class, which ennodes and decodes an amino acid sequence \n",
    "\n",
    "class Tokenizer:\n",
    "    ''' \n",
    "    To encode and decode any amino acid string\n",
    "    '''\n",
    "    # class attribute \n",
    "    amino_acids = amino_acids_list\n",
    "\n",
    "    def __init__(self, special_tokens = Iterable[str]):\n",
    "        # define a vocab\n",
    "        self.vocab = Tokenizer.amino_acids + list(special_tokens)\n",
    "        # mapping each vocab to a token (a numeric value)\n",
    "        self.token2idx = {token:i for i, token in enumerate(self.vocab)} \n",
    "        # mapping numeric value back to a token\n",
    "        self.idx2token = {i:token for token, i  in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, inputs: Iterable[str]) -> Iterable[int]:\n",
    "        return [self.token2idx[token] for token in inputs]\n",
    "    \n",
    "    def decode(self, inputs: Iterable[int]) -> Iterable[str]:\n",
    "        return [self.idx2token[idx] for idx in inputs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 amino acids         : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n",
      "First 20 encoded amino acids : [20, 20, 20, 18, 6, 13, 3, 20, 20, 18, 18, 8, 18, 10, 14, 15, 2, 6, 0, 2]\n",
      "First 20 decoded amino acids : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n"
     ]
    }
   ],
   "source": [
    "# creating an instance of the Tokenizer. \n",
    "amino_acid_tokenizer = Tokenizer(special_tokens=[\"-\", \"[MASK]\"])\n",
    "\n",
    "# let's encode the first amino-acid-sequence and see the first 10 positions\n",
    "print(f\"First 20 amino acids         : {[i for i in amino_acid_sequences[0][0:20]]}\")\n",
    "print(f\"First 20 encoded amino acids : {amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20]}\")\n",
    "print(f\"First 20 decoded amino acids : {amino_acid_tokenizer.decode(amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'K': 0, 'R': 1, 'G': 2, 'F': 3, 'Y': 4, 'I': 5, 'S': 6, 'C': 7, 'M': 8, 'D': 9, 'W': 10, 'E': 11, 'T': 12, 'Q': 13, 'V': 14, 'P': 15, 'H': 16, 'N': 17, 'L': 18, 'A': 19, '-': 20, '[MASK]': 21}\n"
     ]
    }
   ],
   "source": [
    "print(amino_acid_tokenizer.token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating a Tensor for all amino-seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{116}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making sure that the size of each amino-acid-seq is same\n",
    "\n",
    "len_amino_acid_seq = set()\n",
    "for seq in amino_acid_sequences:\n",
    "    len_amino_acid_seq.add(len(seq))\n",
    "\n",
    "# this set should have only one value \n",
    "len_amino_acid_seq\n",
    "# perfect! all the seq are 116 character long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_amino_acids_tensor(amino_acid_sequences:list, my_tokenizer:Tokenizer):\n",
    "\n",
    "    amino_acid_tensors = []\n",
    "\n",
    "    for seq in amino_acid_sequences:\n",
    "        amino_acid_tensors.append(torch.Tensor(my_tokenizer.encode(seq)).to(torch.int64))\n",
    "\n",
    "    # stacking them \n",
    "    stacked_tensor =  torch.stack(amino_acid_tensors)\n",
    "\n",
    "    return stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amino_acids_tensor = create_amino_acids_tensor(amino_acid_sequences, amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1001, 116])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amino_acids_tensor.shape\n",
    "\n",
    "# 1001 seqs, each with the length of 116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Training data \n",
    "\n",
    "- So what we need is to mask a random position in seq\n",
    "- let's only mask one posiiton as of now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MaskedAminoSeqDataset(Dataset):\n",
    "    def __init__(self, input_tensor: torch.Tensor, mask_token: int):\n",
    "            \"\"\"\n",
    "            Dataset for masked amino acid sequence prediction.\n",
    "\n",
    "            Args:\n",
    "            input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length).\n",
    "            mask_token (int): The token used for masking.\n",
    "            \"\"\"\n",
    "            self.input_tensor = input_tensor\n",
    "            self.mask_token = mask_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_tensor.shape[0] \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seqs, target_amino_acids, mask_positions = \\\n",
    "            self._create_training_data(self.input_tensor, batch_size=1, mask_token=self.mask_token)\n",
    "        return input_seqs.squeeze(0), target_amino_acids.squeeze(0), mask_positions.squeeze(0)\n",
    "\n",
    "    def _create_training_data(self, input_tensor: torch.Tensor, batch_size: int, mask_token: int):\n",
    "        \"\"\"\n",
    "        Creates masked training data efficiently using vectorized operations.\n",
    "\n",
    "        Args:\n",
    "        input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length)\n",
    "        batch_size (int): The desired batch size.\n",
    "        mask_token (int): The token used for masking.\n",
    "\n",
    "        Returns:\n",
    "        tuple: (input_seqs, target_amino_acids, mask_positions)\n",
    "            - input_seqs: Tensor of shape (batch_size, sequence_length) with masked sequences.\n",
    "            - target_amino_acids: Tensor of shape (batch_size,) containing the masked amino acids.\n",
    "            - mask_positions: Tensor of shape (batch_size,) indicating mask positions.\n",
    "        \"\"\"\n",
    "        rows = input_tensor.shape[0]\n",
    "        seq_len = input_tensor.shape[1]\n",
    "        # Randomly select 'batch_size' rows (amino acid sequences)\n",
    "        idx = torch.randint(rows, size=(batch_size,))\n",
    "        input_seqs = input_tensor[idx].clone()\n",
    "\n",
    "        # Generate random mask positions within each selected sequence\n",
    "        mask_positions = torch.randint(seq_len, size=(batch_size, 1))\n",
    "\n",
    "        # Get the target amino acids at the mask positions\n",
    "        target_amino_acids = input_seqs.gather(1, mask_positions).squeeze()\n",
    "\n",
    "        # Create a mask for the selected positions \n",
    "        mask = torch.zeros(input_seqs.size(), dtype=torch.bool)\n",
    "        mask.scatter_(1, mask_positions, 1)\n",
    "\n",
    "        # Apply the mask to replace the target positions with the mask_token\n",
    "        input_seqs[mask] = mask_token\n",
    "\n",
    "        return input_seqs, target_amino_acids, mask_positions.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token id for the MASK\n",
    "amino_acid_tokenizer.encode([\"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming input_tensor is your tensor of amino acid sequences\n",
    "masked_amino_seq_dataset = MaskedAminoSeqDataset(all_amino_acids_tensor, mask_token=21) \n",
    "masked_amino_seq_dataloader = DataLoader(masked_amino_seq_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amino seqs with masked: \n",
      " shape: torch.Size([32, 116]) \n",
      " tensor([[ 8, 14,  6,  ..., 17,  4, 15],\n",
      "        [20, 20, 20,  ...,  6,  9, 15],\n",
      "        [ 8, 11, 12,  ...,  6,  6, 15],\n",
      "        ...,\n",
      "        [14,  1, 14,  ...,  6,  9, 15],\n",
      "        [ 8, 10,  6,  ..., 11,  6, 15],\n",
      "        [ 8,  2,  6,  ..., 17, 20, 20]])\n",
      "targets amino acid:  \n",
      " shape: torch.Size([32]) \n",
      "tensor([12,  6,  6, 18,  6, 20, 13, 11, 18, 19, 10,  6, 14,  2, 13,  6, 18, 10,\n",
      "        12, 18,  9,  2, 14, 13,  9,  4,  2,  4,  5, 13, 12,  2])\n",
      "mask posittions:  \n",
      " shape: torch.Size([32]) \n",
      "tensor([ 76,  31,  47,  53,  39, 114,  62,  36,  74,  70,  13,  45,  95,  61,\n",
      "         58,  76,   6,  55,  92,  93,  36,  86,  38, 110,  90, 107,   4, 107,\n",
      "         21,  99,  39,  61])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(f\"amino seqs with masked: \\n shape: {i[0].shape} \\n {i[0]}\")\n",
    "    print(f\"targets amino acid:  \\n shape: {i[1].shape} \\n{i[1]}\")\n",
    "    print(f\"mask posittions:  \\n shape: {i[2].shape} \\n{i[2]}\")\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings\n",
    "\n",
    "We need to embeddings\n",
    "\n",
    "- amino acid embeddings \n",
    "- position embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, embed_size)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, max_seq_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.token = torch.nn.Embedding(vocab_size, embed_size, dtype=torch.float32)\n",
    "        self.position = SinusoidalPositionEncoding(embed_size, max_seq_length=max_seq_length)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        word_embed = self.token(x) \n",
    "        pos_embed = self.position(x)\n",
    "        out = word_embed + pos_embed\n",
    "\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(amino_acid_tokenizer)\n",
    "d_model = 64 # embedding size \n",
    "max_seq_length = masked_amino_seq_dataset.input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = BERTEmbeddings(vocab_size=vocab_size, embed_size=d_model, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape:     torch.Size([32, 116]) \n",
      "embedded batch shape: torch.Size([32, 116, 64])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(f\"input batch shape:     {i[0].shape} \")\n",
    "\n",
    "    print(f\"embedded batch shape: {test_emb(i[0]).shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi Headed Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.query = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        self.key = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        self.value = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, d_model)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "            # Note: mask if not used, it is mainly to tell attention the locations on which \n",
    "                it should not learn much, like padding indexes\n",
    "                - we dont have padding here as of now, so no need it. \n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, d_model)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
    "\n",
    "        # to mask the pads (diff from the other mask) so the attention does not learn from it\n",
    "        # # fill 0 mask with super small number so it wont affect the softmax weight\n",
    "        # # (batch_size, h, max_len, max_len)\n",
    "        # scores = scores.masked_fill(mask == 0, -1e9)    \n",
    "\n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        # softmax to put attention weight for all non-pad tokens\n",
    "        # max_len X max_len matrix of attention\n",
    "        weights = F.softmax(scores, dim=-1)           \n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "        # (batch_size, max_len, d_model)\n",
    "        return self.output_linear(context)\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model=768,\n",
    "        heads=12, \n",
    "        feed_forward_hidden=768 * 4, \n",
    "        dropout=0.1\n",
    "        ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        # residual layer\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        # bottleneck\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = MultiHeadedAttention(heads = 16, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape:     torch.Size([32, 116]) \n",
      "mask posiitons shape:   torch.Size([32])\n",
      "embedded batch shape: torch.Size([32, 116, 64])\n",
      "The output from the Attention : torch.Size([32, 116, 64])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(f\"input batch shape:     {i[0].shape} \")\n",
    "    print(f\"mask posiitons shape:   {i[2].shape}\")\n",
    "\n",
    "    print(f\"embedded batch shape: {test_emb(i[0]).shape}\")\n",
    "    embded = test_emb(i[0])\n",
    "    mask = i[2]\n",
    "    attention_output = heads(embded, embded, embded,  mask)\n",
    "\n",
    "    print(f\"The output from the Attention : {attention_output.shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output from multiheaded attention goes through a little bit of forward passes, because why not!! \n",
    "- so below is a simple Feedforward pass code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer\n",
    "\n",
    "- putting all together, \n",
    "\n",
    "- embedded matrix comes, first it goes to Attention module \n",
    "- Then layer normalization \n",
    "- then a feed forward part \n",
    "- and it again goes through a layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model=768,\n",
    "        heads=12, \n",
    "        feed_forward_hidden=768 * 4, \n",
    "        dropout=0.1\n",
    "        ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(d_model, dtype=torch.float32)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        # residual layer\n",
    "        interacted = interacted.to(torch.float32)\n",
    "        embeddings = embeddings.to(torch.float32)\n",
    "\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        # bottleneck\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, max_seq_length=500, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = d_model * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbeddings(vocab_size=vocab_size, embed_size=d_model, max_seq_length=max_seq_length)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.encoder_blocks = torch.nn.ModuleList(\n",
    "            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # attention masking for padded token\n",
    "\n",
    "        # (batch_size, 1, seq_len, seq_len)\n",
    "        # mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        # as of now mask has no role to play, it's for not paying attention to Padding idx\n",
    "        mask = torch.Tensor([1])\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder.forward(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(amino_acid_tokenizer)\n",
    "d_model = 64 # embedding size \n",
    "max_seq_length = masked_amino_seq_dataset.input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_encoder_test = BERT(vocab_size=vocab_size, d_model=d_model, n_layers=3, heads=4, max_seq_length=116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape:     torch.Size([32, 116]) \n",
      "mask posiitons shape:   torch.Size([32])\n",
      "bert encoder output shape: torch.Size([32, 116, 64])\n"
     ]
    }
   ],
   "source": [
    "# let's see if we can run oe forward pass\n",
    "\n",
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(f\"input batch shape:     {i[0].shape} \")\n",
    "    print(f\"mask posiitons shape:   {i[2].shape}\")\n",
    "\n",
    "    # print(f\"embedded batch shape: {test_emb(i[0]).shape}\")\n",
    "    # embded = test_emb(i[0])\n",
    "    # mask = i[2]\n",
    "    # attention_output = heads(embded, embded, embded,  mask)\n",
    "\n",
    "    # print(f\"The output from the Attention : {attention_output.shape}\")\n",
    "\n",
    "    bert_output = bert_encoder_test(i[0])\n",
    "    print(f\"bert encoder output shape: {bert_output.shape}\")\n",
    "\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Loss Func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Traning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
