{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model for understanding mutation in protien sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "\n",
    "import string\n",
    "from typing import Iterable, Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---LSQF--LLMLWVPGSKGEIVLTQSPASVSVSPGERVTISCQASESVGNTYLNWLQQKSGQSPRWLIYQVSKLESGIPARFRGSGSGTDFTFTISRVEAEDVAHYYSQQ-----',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPALVSVSPGERVTISCKASQSVGNTYLSWFRQKPGQSPRGLIYKVSNLPSGVPSRFRGSGAEKDFTLTISRVEAVDGAVYYCAQASYSP',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPASVSVSPGERVTISCKASQSLGNTYLHWFQQKPGQSPRRLIYQVSNLLSGVPSRFSGSGAGKDFSLTISSVEAGDGAVYYCFQGSYDP']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'X_set.txt'\n",
    "\n",
    "# Initialize lists to hold the phylogenetic position strings and amino acid sequences\n",
    "specie_code = []\n",
    "amino_acid_sequences = []\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        specie_code.append(parts[0])\n",
    "        amino_acid_sequences.append(parts[1])\n",
    "\n",
    "amino_acid_sequences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amino_acid_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would need two more set of information \n",
    "\n",
    "# 1. protien type \n",
    "protein_types = ['A1'] * len(amino_acid_sequences)\n",
    "\n",
    "# 2. weights for species\n",
    "specie_weight = torch.rand(len(amino_acid_sequences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tokenizer\n",
    "\n",
    "- There are 20 amino acids, each letter in the chain represents one of them. \n",
    "- Converting them into 20 tokens, meaning each amino acid would get a number associated with it. \n",
    "- Would also need a special character token, which is \"-\", something related to multiple-sequence-alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Amino Acids: 20\n"
     ]
    }
   ],
   "source": [
    "# making sure that there are only 20 diff types of amino acids \n",
    "\n",
    "amino_acid_set = set()\n",
    "\n",
    "for seq in amino_acid_sequences:\n",
    "    for acid in seq:\n",
    "        if acid != \"-\":\n",
    "            amino_acid_set.add(acid)\n",
    "\n",
    "# 20 amino acids\n",
    "print(f\"Num of Amino Acids: {len(amino_acid_set) }\")\n",
    "amino_acids_list = list(amino_acid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Tokenzer class, which ennodes and decodes an amino acid sequence \n",
    "\n",
    "class AminoAcidTokenizer:\n",
    "    ''' \n",
    "    To encode and decode any amino acid string\n",
    "    '''\n",
    "    # class attribute\n",
    "    # all 20 types of amino acids\n",
    "    amino_acids = ['S','D','H','L','T','E','W','N','Y','Q','C','G','V','K','I','R','M','F','A','P']\n",
    "\n",
    "    def __init__(self, special_tokens: Optional[Iterable[str]] = None):\n",
    "        # define a vocab\n",
    "        self.vocab = AminoAcidTokenizer.amino_acids\n",
    "        if special_tokens:\n",
    "            self.vocab += list(special_tokens)\n",
    "        # mapping each vocab to a token (a numeric value)\n",
    "        self.token2idx = {token:i for i, token in enumerate(self.vocab)} \n",
    "        # mapping numeric value back to a token\n",
    "        self.idx2token = {i:token for token, i  in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, inputs: Iterable[str]) -> Iterable[int]:\n",
    "        return [self.token2idx[token] for token in inputs]\n",
    "    \n",
    "    def decode(self, inputs: Iterable[int]) -> Iterable[str]:\n",
    "        return [self.idx2token[idx] for idx in inputs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 amino acids         : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n",
      "First 20 encoded amino acids : [20, 20, 20, 3, 0, 9, 17, 20, 20, 3, 3, 16, 3, 6, 12, 19, 11, 0, 13, 11]\n",
      "First 20 decoded amino acids : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n"
     ]
    }
   ],
   "source": [
    "# creating an instance of the Tokenizer. \n",
    "amino_acid_tokenizer = AminoAcidTokenizer(special_tokens=[\"-\", \"[MASK]\", \"[PAD]\"])\n",
    "\n",
    "# let's encode the first amino-acid-sequence and see the first 10 positions\n",
    "print(f\"First 20 amino acids         : {[i for i in amino_acid_sequences[0][0:20]]}\")\n",
    "print(f\"First 20 encoded amino acids : {amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20]}\")\n",
    "print(f\"First 20 decoded amino acids : {amino_acid_tokenizer.decode(amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S': 0, 'D': 1, 'H': 2, 'L': 3, 'T': 4, 'E': 5, 'W': 6, 'N': 7, 'Y': 8, 'Q': 9, 'C': 10, 'G': 11, 'V': 12, 'K': 13, 'I': 14, 'R': 15, 'M': 16, 'F': 17, 'A': 18, 'P': 19, '-': 20, '[MASK]': 21, '[PAD]': 22}\n"
     ]
    }
   ],
   "source": [
    "# all tokens mapped to an idx\n",
    "print(amino_acid_tokenizer.token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_acid_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to Amino Acid tokenizers, creating Protien tokenizer\n",
    "\n",
    "class ProteinTokenizer:\n",
    "    '''\n",
    "    To encode and decode protein types and amino acid sequences\n",
    "    '''\n",
    "    # class attribute\n",
    "    protiens = ['A1', 'A2']\n",
    "\n",
    "    def __init__(self, special_tokens: Iterable[str] = None):\n",
    "        # define a vocab\n",
    "        self.vocab = ProteinTokenizer.protiens \n",
    "        if special_tokens:   \n",
    "            self.vocab += list(special_tokens)\n",
    "        # mapping each vocab to a token (a numeric value)\n",
    "        self.token2idx = {token:i for i, token in enumerate(self.vocab)} \n",
    "        # mapping numeric value back to a token\n",
    "        self.idx2token = {i:token for token, i  in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, inputs: Iterable[str]) -> Iterable[int]:\n",
    "        return [self.token2idx[token] for token in inputs]\n",
    "    \n",
    "    def decode(self, inputs: Iterable[int]) -> Iterable[str]:\n",
    "        return [self.idx2token[idx] for idx in inputs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A1': 0, 'A2': 1}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_tokenizer = ProteinTokenizer()\n",
    "protein_tokenizer.token2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing all protein seq and protein types \n",
    "def create_encoded_tensors(amino_acid_sequences: list, protein_types: list, \n",
    "                           amino_acid_tokenizer: AminoAcidTokenizer, \n",
    "                           protein_tokenizer: ProteinTokenizer):\n",
    "    \n",
    "    amino_acid_tensors = []\n",
    "    protein_type_tensors = []\n",
    "\n",
    "    for seq, p_type in zip(amino_acid_sequences, protein_types):\n",
    "        amino_acid_tensors.append(torch.tensor(amino_acid_tokenizer.encode(seq), dtype=torch.int64))\n",
    "        protein_type_tensors.append(torch.tensor(protein_tokenizer.encode([p_type]), dtype=torch.int64))\n",
    "\n",
    "    return amino_acid_tensors, protein_type_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_amino_acids, encoded_protein_types = create_encoded_tensors(\n",
    "    amino_acid_sequences, \n",
    "    protein_types, \n",
    "    amino_acid_tokenizer, \n",
    "    protein_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "# both list to have same len\n",
    "print(len(encoded_amino_acids))\n",
    "print(len(encoded_protein_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MaskedAminoSeqDataset(Dataset):\n",
    "    def __init__(self, encoded_amino_acids: list,\n",
    "                encoded_protein_types: list,\n",
    "                specie_weight : torch.Tensor,\n",
    "                mask_token: int,\n",
    "                pad_token: int,\n",
    "                max_len: int ):\n",
    "            \"\"\"\n",
    "            Dataset for masked amino acid sequence prediction.\n",
    "\n",
    "            Args:\n",
    "            input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length).\n",
    "            mask_token (int): The token used for masking.\n",
    "            \"\"\"\n",
    "            self.encoded_amino_acids = encoded_amino_acids\n",
    "            self.encoded_protein_types = encoded_protein_types\n",
    "            self.specie_weight = specie_weight\n",
    "            self.mask_token = mask_token\n",
    "            self.pad_token = pad_token\n",
    "            self.max_len = max_len\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_amino_acids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seqs, target_amino_acids, mask_positions, encoded_protien, sample_weight = \\\n",
    "            self._create_training_data(self.encoded_amino_acids[idx],\n",
    "                                        self.encoded_protein_types[idx],\n",
    "                                        self.specie_weight[idx],\n",
    "                                        self.mask_token,\n",
    "                                        self.pad_token,\n",
    "                                        self.max_len, \n",
    "                                        )\n",
    "        \n",
    "        return input_seqs.squeeze(0), target_amino_acids.squeeze(0), mask_positions.squeeze(0), encoded_protien, sample_weight\n",
    "\n",
    "\n",
    "    def _create_training_data(self, encoded_amino_acid: torch.Tensor,\n",
    "                            encoded_protein_type: torch.Tensor, \n",
    "                            specie_weight: torch.Tensor, \n",
    "                            mask_token: int,\n",
    "                            pad_token: int,\n",
    "                            max_len: int,  \n",
    "                            min_masks: int = 1,\n",
    "                            max_masks: int = 5):\n",
    "        \"\"\"\n",
    "        Create training data for masked amino acid sequence prediction.\n",
    "\n",
    "        This function takes an encoded amino acid sequence and applies random masking\n",
    "        to create input-target pairs for training a BERT-like model. It also handles\n",
    "        padding or truncation to ensure consistent sequence length.\n",
    "\n",
    "        Args:\n",
    "            encoded_amino_acid (torch.Tensor): Encoded amino acid sequence.\n",
    "            encoded_protein_type (torch.Tensor): Encoded protein type.\n",
    "            specie_weight (torch.Tensor): Weight associated with the species.\n",
    "            mask_token (int): Token used for masking.\n",
    "            pad_token (int): Token used for padding.\n",
    "            max_len (int): Maximum length of the sequence.\n",
    "            min_masks (int, optional): Minimum number of tokens to mask. Defaults to 1.\n",
    "            max_masks (int, optional): Maximum number of tokens to mask. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - masked_seq (torch.Tensor): Input sequence with masked tokens.\n",
    "                - target_seq (torch.Tensor): Target sequence for masked token prediction.\n",
    "                - fixed_mask_positions (torch.Tensor): Fixed-size tensor of mask positions.\n",
    "                - encoded_protein_type (torch.Tensor): Encoded protein type.\n",
    "                - specie_weight (torch.Tensor): Weight associated with the species.\n",
    "\n",
    "        Notes:\n",
    "            - The function pads or truncates the input sequence to `max_len`.\n",
    "            - It randomly masks between `min_masks` and `max_masks` tokens.\n",
    "            - The `fixed_mask_positions` tensor has a fixed size of `max_masks`,\n",
    "            with -1 values indicating unused mask positions.\n",
    "            - Target sequences use -100 for non-masked positions (ignored in loss calculation).\n",
    "        \"\"\"\n",
    "        # Pad or truncate the sequence to max_len\n",
    "        seq_len = encoded_amino_acid.shape[0]\n",
    "        if seq_len < max_len:\n",
    "            padding = torch.full((max_len - seq_len,), pad_token, dtype=encoded_amino_acid.dtype)\n",
    "            input_seq = torch.cat([encoded_amino_acid, padding])\n",
    "        else:\n",
    "            input_seq = encoded_amino_acid[:max_len]\n",
    "        \n",
    "        # Determine number of masks\n",
    "        num_masks = torch.randint(min_masks, min(max_masks + 1, max_len + 1), (1,)).item()\n",
    "        \n",
    "        # Create mask positions\n",
    "        mask_positions = torch.randperm(max_len)[:num_masks]\n",
    "        # Create a fixed-size tensor for mask positions to make sure it's same size vector\n",
    "        fixed_mask_positions = torch.full((max_masks,), -1, dtype=torch.long)\n",
    "        fixed_mask_positions[:num_masks] = mask_positions    \n",
    "        \n",
    "        # Create masked input sequence\n",
    "        masked_seq = input_seq.clone()\n",
    "        masked_seq[mask_positions] = mask_token\n",
    "        \n",
    "        # Create target sequence\n",
    "        target_seq = torch.full((max_len,), -100, dtype=input_seq.dtype)  # -100 is often used to ignore in loss\n",
    "        target_seq[mask_positions] = input_seq[mask_positions]\n",
    "        \n",
    "        # Ensure encoded_protein_type is the right shape\n",
    "        if encoded_protein_type.dim() == 0:\n",
    "            encoded_protein_type = encoded_protein_type.unsqueeze(0)\n",
    "        \n",
    "        return masked_seq, target_seq, fixed_mask_positions, encoded_protein_type, specie_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming input_tensor is your tensor of amino acid sequences\n",
    "masked_amino_seq_dataset = MaskedAminoSeqDataset(\n",
    "    encoded_amino_acids=encoded_amino_acids, \n",
    "    encoded_protein_types=encoded_protein_types, \n",
    "    specie_weight=specie_weight,\n",
    "    mask_token=21,\n",
    "    pad_token=22, \n",
    "    max_len=116\n",
    ") \n",
    "masked_amino_seq_dataloader = DataLoader(masked_amino_seq_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amino seqs with masked: shape: torch.Size([32, 116])\n",
      "targets amino acid:     shape: torch.Size([32, 116])\n",
      "mask posittions:        shape: torch.Size([32, 5]) \n",
      "encoded protein type:   shpae: torch.Size([32, 1])\n",
      "specie_weight:          shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "i, t, m, n, q = 0, 0, 0, 0, 0\n",
    "for data in masked_amino_seq_dataloader:\n",
    "    print(f\"amino seqs with masked: shape: {data[0].shape}\")\n",
    "    print(f\"targets amino acid:     shape: {data[1].shape}\")\n",
    "    print(f\"mask posittions:        shape: {data[2].shape} \")\n",
    "    print(f\"encoded protein type:   shpae: {data[3].shape}\")\n",
    "    print(f\"specie_weight:          shape: {data[4].shape}\")\n",
    "\n",
    "    i = data[0]\n",
    "    t = data[1]\n",
    "    m = data[2]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- amino acid embeddings \n",
    "- position embeddings \n",
    "- protein type embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding module.\n",
    "\n",
    "    This module generates sinusoidal position embeddings for input sequences.\n",
    "    It can create up to `max_seq_length` unique position embeddings (default 5000).\n",
    "\n",
    "    Args:\n",
    "        embed_size (int): The size of each embedding vector.\n",
    "        max_seq_length (int, optional): The maximum sequence length to support. \n",
    "            Defaults to 5000.\n",
    "\n",
    "    Attributes:\n",
    "        embed_size (int): The size of each embedding vector.\n",
    "        pe (Tensor): The pre-computed position encoding matrix of shape \n",
    "            (1, max_seq_length, embed_size).\n",
    "\n",
    "    Note:\n",
    "        - The actual number of unique embeddings used depends on the input \n",
    "          sequence length in the forward pass.\n",
    "        - While there are `max_seq_length` distinct vectors, positions beyond \n",
    "          this could theoretically be represented due to the periodic nature \n",
    "          of sine and cosine functions, albeit with some loss of uniqueness.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, embed_size)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, amino_vocab_size, protien_vocab_size, embed_size, max_seq_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.amino_acid_token = torch.nn.Embedding(amino_vocab_size, embed_size, dtype=torch.float32)\n",
    "        self.position = SinusoidalPositionEncoding(embed_size, max_seq_length=max_seq_length)\n",
    "        self.protien_token = torch.nn.Embedding(protien_vocab_size, embed_size, dtype=torch.float32)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, amino_acid_seqs, protiens):\n",
    "        \"\"\"\n",
    "        amino_acid_seqs = (B * C) ; protien =  (32 * 1)\n",
    "        output ===> (B * C * d_model)\n",
    "        \"\"\"\n",
    "    \n",
    "        amino_acid_embed = self.amino_acid_token(amino_acid_seqs) \n",
    "        pos_embed = self.position(amino_acid_seqs)\n",
    "        protien_embed = self.protien_token(protiens)\n",
    "        out = amino_acid_embed + pos_embed + protien_embed\n",
    "\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "amino_vocab_size = len(amino_acid_tokenizer)\n",
    "protein_vocab_size = len(protein_tokenizer)\n",
    "d_model = 64 # embedding size \n",
    "max_seq_length = 200 # this doen't have to be precise, this is only for positional encoding\n",
    "\n",
    "\n",
    "test_emb = BERTEmbeddings(amino_vocab_size=amino_vocab_size,\n",
    "                        protien_vocab_size=protein_vocab_size,\n",
    "                        embed_size=d_model,\n",
    "                        max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape:     torch.Size([32, 116]) \n",
      "embedded batch shape: torch.Size([32, 116, 64])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "for data in masked_amino_seq_dataloader:\n",
    "    print(f\"input batch shape:     {data[0].shape} \")\n",
    "\n",
    "    print(f\"embedded batch shape: {test_emb(data[0], data[3]).shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
